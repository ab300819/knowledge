<!-- TOC -->

- [软件磁盘阵列 Software RAID](#软件磁盘阵列-software-raid)
    - [软件磁盘阵列的设置](#软件磁盘阵列的设置)
    - [14.2.4 仿真 RAID 错误的救援模式](#1424-仿真-raid-错误的救援模式)
    - [14.2.5 开机自动启动 RAID 并自动挂载](#1425-开机自动启动-raid-并自动挂载)
    - [14.2.6 关闭软件 RAID](#1426-关闭软件-raid)

<!-- /TOC -->

## 软件磁盘阵列 Software RAID

### 软件磁盘阵列的设置

```shell
mdadm --detail /dev/md0
mdadm --create /dev/md[0-9] --auto=yes --level=[015] --chunk=NK \
# > --raid-devices=N --spare-devices=N /dev/sdx /dev/hdx...
```

选项与参数：

- `--create` 为创建 RAID 的选项；
- `--auto=yes` 决定创建后面接的软件磁盘阵列装置，亦即 `/dev/md0`, `/dev/md1...`
- `--raid-devices=N` 使用几个磁碟 (partition) 作为磁盘阵列的装置
- `--spare-devices=N` 使用几个磁碟作为备用 (spare) 装置
- `--level=[015]` 配置这组磁盘阵列的等级。支持很多，不过建议只要用 `0`, `1`, `5` 即可
- `--detail` 后面所接的那个磁盘阵列装置的详细资讯

**以 mdadm 创建 RAID**

```shell
[root@www ~]# mdadm --create /dev/md0 --auto=yes --level=5 --chunk=256K \
> --raid-devices=4 --spare-devices=1 /dev/vda{5,6,7,8,9}
# 详细的参数说明请回去前面看看罗！这里我透过 {} 将重复的项目简化！
```

```
[root@www ~]# mdadm --detail /dev/md0
/dev/md0:                                        <==RAID 装置档名
        Version : 00.90.03
  Creation Time : Tue Mar 10 17:47:51 2009       <==RAID 被创建的时间
     Raid Level : raid5                          <==RAID 等级为 RAID 5
     Array Size : 2963520 (2.83 GiB 3.03 GB)     <==此 RAID 的可用磁碟容量
  Used Dev Size : 987840 (964.85 MiB 1011.55 MB) <==每个装置的可用容量
   Raid Devices : 4                              <==用作 RAID 的装置数量
  Total Devices : 5                              <==全部的装置数量
Preferred Minor : 0
    Persistence : Superblock is persistent

    Update Time : Tue Mar 10 17:52:23 2009
          State : clean
 Active Devices : 4                              <==启动的(active)装置数量
Working Devices : 5                              <==可动作的装置数量
 Failed Devices : 0                              <==出现错误的装置数量
  Spare Devices : 1                              <==预备磁碟的数量

         Layout : left-symmetric
     Chunk Size : 64K      <==就是图2.1.4内的小区块

           UUID : 7c60c049:57d60814:bd9a77f1:57e49c5b <==此装置(RAID)识别码
         Events : 0.2

    Number   Major   Minor   RaidDevice State
       0       3        6        0      active sync   /dev/hda6
       1       3        7        1      active sync   /dev/hda7
       2       3        8        2      active sync   /dev/hda8
       3       3        9        3      active sync   /dev/hda9

       4       3       10        -      spare   /dev/hda10
# 最后五行就是这五个装置目前的情况，包括四个 active sync 一个 spare ！
# 至於 RaidDevice  指的则是此 RAID 内的磁碟顺序
```

查看磁盘阵列信息：

```shell
[root@www ~]# cat /proc/mdstat
Personalities : [raid6] [raid5] [raid4]
md0 : active raid5 hda9[3] hda10[4](S) hda8[2] hda7[1] hda6[0]    <==第一行
      2963520 blocks level 5, 64k chunk, algorithm 2 [4/4] [UUUU] <==第二行

unused devices: <none>
```
1. 第一行部分：指出 md0 为 raid5 ，且使用了 hda9, hda8, hda7, hda6 等四颗磁碟装置。每个装置后面的中括号 [] 内的数字为此磁碟在 RAID 中的顺序 (RaidDevice)；至於 hda10 后面的 [S] 则代表 hda10 为 spare 之意。

2. 第二行：此磁盘阵列拥有 2963520 个block(每个 block 单位为 1K)，所以总容量约为 3GB， 使用 RAID 5 等级，写入磁碟的小区块 (chunk) 大小为 64K，使用 algorithm 2 磁盘阵列演算法。 [m/n] 代表此阵列需要 m 个装置，且 n 个装置正常运行。因此本 md0 需要 4 个装置且这 4 个装置均正常运行。 后面的 [UUUU] 代表的是四个所需的装置 (就是 [m/n] 里面的 m) 的启动情况，U 代表正常运行，若为 _ 则代表不正常。


**格式化与挂载使用 RAID**

```shell
[root@www ~]# mkfs.xfs -f -d su=256k,sw=3 -r extsize=768k /dev/md0
# 有趣吧！是 /dev/md0 做为装置被格式化呢！

[root@www ~]# mkdir /srv/raid
[root@www ~]# mount /dev/md0 /srv/raid
[root@www ~]# df -Th /srv/raid
Filesystem  Type  Size  Used  Avail Use%  Mounted on
/dev/md0    xfs   3.0G  33M   3.0G  2%    /srv/raid
```

### 14.2.4 仿真 RAID 错误的救援模式

```shell
[root@study ~]# mdadm --manage /dev/md[0-9] [--add 装置] [--remove 装置] [--fail 装置]
选项与参数：
--add       ：会将后面的装置加入到这个 md 中！
--remove    ：会将后面的装置由这个 md 中移除
--fail      ：会将后面的装置设定成为出错的状态
```

**配置磁盘为错误 (fault)**

```shell
# 0. 先复制一些东西到 /srv/raid 去，假设这个 RAID 已经在使用了
[root@study ~]# cp -a /etc /var/log /srv/raid
[root@study ~]# df -Th /srv/raid ; du -sm /srv/raid/*
Filesystem  Type  Size  Used  Avail Use%  Mounted on
/dev/md0    xfs   3.0G  144M  2.9G  5%    /srv/raid
28    /srv/raid/etc <==看吧！确实有资料在里面喔！
51    /srv/raid/log

# 1. 假设 /dev/vda7 这个装置出错了！实际模拟的方式：
[root@study ~]# mdadm --manage /dev/md0 --fail /dev/vda7
mdadm: set /dev/vda7 faulty in /dev/md0      # 设定成为错误的装置啰！
/dev/md0:
.....(中間省略).....
    Update Time : Mon Jul 27 15:32:50 2015
          State : clean, degraded, recovering
 Active Devices : 3
Working Devices : 4
 Failed Devices : 1      <==出错的磁盘有一个！
  Spare Devices : 1
.....(中間省略).....

    Number   Major   Minor   RaidDevice State
       0     252        5        0      active sync   /dev/vda5
       1     252        6        1      active sync   /dev/vda6
       4     252        9        2      spare rebuilding   /dev/vda9
       5     252        8        3      active sync   /dev/vda8

       2     252        7        -      faulty   /dev/vda7
# 这的动作要快做才会看到！ /dev/vda9 启动了而 /dev/vda7 死掉了
```

```
# 2. 已经藉由 spare disk 重建完毕的 RAID 5 情况
[root@study ~]# mdadm --detail /dev/md0
....(前面省略)....
    Number   Major   Minor   RaidDevice State
       0     252        5        0      active sync   /dev/vda5
       1     252        6        1      active sync   /dev/vda6
       4     252        9        2      active sync   /dev/vda9
       5     252        8        3      active sync   /dev/vda8

       2     252        7        -      faulty   /dev/vda7
```

**将出错的磁盘删除并加入新磁盘**

1. 先从 `/dev/md0` 数组中移除 `/dev/vda7` 这块磁盘
2. 整个 Linux 系统关机，拔出 `/dev/vda7` 这块磁盘，并安装上新的 `/dev/vda7` 磁盘，之后开机
3. 将新的 `/dev/vda7` 放入 `/dev/md0` 数组当中！

```shell
# 3. 拔除『旧的』/dev/vda7 磁盘
[root@study ~]# mdadm --manage /dev/md0 --remove /dev/vda7
# 假设接下来你就进行了上面谈到的第 2, 3 个步骤，然后重新启动成功了！

# 4. 安装『新的』/dev/vda7 磁盘
[root@study ~]# mdadm --manage /dev/md0 --add /dev/vda7
[root@study ~]# mdadm --detail /dev/md0
....(前面省略)....
    Number   Major   Minor   RaidDevice State
       0     252        5        0      active sync   /dev/vda5
       1     252        6        1      active sync   /dev/vda6
       4     252        9        2      active sync   /dev/vda9
       5     252        8        3      active sync   /dev/vda8

       6     252        7        -      spare   /dev/vda7
```

### 14.2.5 开机自动启动 RAID 并自动挂载

```shell
[root@www ~]# mdadm --detail /dev/md0 | grep -i uuid
        UUID : 7c60c049:57d60814:bd9a77f1:57e49c5b
# 后面那一串数据，就是这个装置向系统注册的 UUID 识别码！

# 开始配置 mdadm.conf
[root@www ~]# vi /etc/mdadm.conf
ARRAY /dev/md0 UUID=7c60c049:57d60814:bd9a77f1:57e49c5b
#     RAID装置      识别码内容

# 开始设定开机自动挂载并测试
[root@study ~]# blkid /dev/md0
/dev/md0: UUID="494cb3e1-5659-4efc-873d-d0758baec523" TYPE="xfs"

[root@study ~]# vim /etc/fstab
UUID=494cb3e1-5659-4efc-873d-d0758baec523 /srv/raid xfs defaults 0 0

[root@study ~]# umount /dev/md0; mount -a
[root@study ~]# df -Th /srv/raid
Filesystem  Type  Size  Used  Avail Use%  Mounted on
/dev/md0    xfs   3.0G  111M  2.9G  4%    /srv/raid
# 你得确定可以顺利挂载，并且没有发生任何错误
```

### 14.2.6 关闭软件 RAID

```shell
# 1. 先卸除且删除配置文件内与这个 /dev/md0 有关的设定：
[root@study ~]# umount /srv/raid
[root@study ~]# vim /etc/fstab
UUID=494cb3e1-5659-4efc-873d-d0758baec523 /srv/raid xfs defaults 0 0
# 将这一行删除掉！或者是注释掉也可以！

# 2. 先覆盖掉 RAID 的 metadata 以及 XFS 的 superblock，才关闭 /dev/md0 的方法
[root@study ~]# dd if=/dev/zero of=/dev/md0 bs=1M count=50
[root@study ~]# mdadm --stop /dev/md0
mdadm: stopped /dev/md0 <==不啰唆！这样就关闭了！
[root@study ~]# dd if=/dev/zero of=/dev/vda5 bs=1M count=10
[root@study ~]# dd if=/dev/zero of=/dev/vda6 bs=1M count=10
[root@study ~]# dd if=/dev/zero of=/dev/vda7 bs=1M count=10
[root@study ~]# dd if=/dev/zero of=/dev/vda8 bs=1M count=10
[root@study ~]# dd if=/dev/zero of=/dev/vda9 bs=1M count=10

[root@study ~]# cat /proc/mdstat
Personalities : [raid6] [raid5] [raid4]
unused devices: <none> <==看吧！确实不存在任何数组装置！

[root@study ~]# vim /etc/mdadm.conf
#ARRAY /dev/md0 UUID=2256da5f:4870775e:cf2fe320:4dfabbc6
# 删除他或是注释他！
```