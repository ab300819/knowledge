<!-- TOC -->

- [14.3 逻辑卷管理器 Logical Volume Manager](#143-逻辑卷管理器-logical-volume-manager)
    - [14.3.1 一些概念](#1431-一些概念)
    - [14.3.2 LVM 实作流程](#1432-lvm-实作流程)
    - [14.3.3 放大 LV 容量](#1433-放大-lv-容量)
    - [14.3.4 缩小 LV 容量](#1434-缩小-lv-容量)
    - [14.3.5 使用 LVM thin Volume 让 LVM 动态自动调整磁盘使用率](#1435-使用-lvm-thin-volume-让-lvm-动态自动调整磁盘使用率)
    - [14.3.6 LVM 的系统快照](#1436-lvm-的系统快照)
    - [14.3.6 LVM 相关命令汇整与 LVM 的关闭](#1436-lvm-相关命令汇整与-lvm-的关闭)

<!-- /TOC -->

# 14.3 逻辑卷管理器 Logical Volume Manager

## 14.3.1 一些概念

* Physical Volume, PV, 物理卷 <br>
实际的 partition 需要调整系统识别码 (system ID) 成为 8e (LVM 的识别码)，然后再经过 pvcreate 的命令将他转成 LVM 最底层的实体卷轴 (PV) ，之后才能够将这些 PV 加以利用！ 调整 system ID 的方是就是透过 gdisk 啦！

* Volume Group, VG, 卷用户组<br>
所谓的 LVM 大磁盘就是将许 PV 整合成这个 VG, 所以 VG 就是 LVM 组合起来的大磁盘。

* Physical Extend, PE, 物理扩展块<br>
是整个 LVM 最小的储存区块，有点像文件系统里面的block大小

*  Logical Volume, LV, 逻辑卷<br>
最终的VG还会被切成 LV, 这个 LV 就是最后可以被格式化使用的类似分区。  

![image](resources/pe_vg.gif)  
如上图所示，VG 内的 PE 会分给虚线部分的 LV，如果未来这个 VG 要扩充的话，加上其他的 PV 即可。 而最重要的 LV 如果要扩充的话，也是透过加入 VG 内没有使用到的 PE 来扩充的！

**实现流程**

![image](resources/centos7_lvm.jpg)  

* 线性模式 (linear)：假如我将 /dev/vda1, /dev/vdb1 这两个 partition 加入到 VG 当中，并且整个 VG 只有一个 LV 时，那么所谓的线性模式就是：当 /dev/vda1 的容量用完之后，/dev/vdb1 的硬盘才会被使用到， 这也是我们所建议的模式。
* 交错模式 (triped)：就是将一笔数据拆成两部分，分别写入 /dev/vda1 与 /dev/vdb1 的意思，感觉上有点像 RAID 0 啦！如此一来，一份数据用两颗硬盘来写入，理论上，读写的效能会比较好。

LVM最主要的用处是在实现一个可以弹性调整容量的文件系统上，而不是在新建一个性能为主的磁盘上。因此，LVM 默认的读写模式是线性模式。

## 14.3.2 LVM 实作流程

* 使用 4 个 partition ，每个 partition 的容量均为 1GB 左右，且 system ID 需要为 8e；
* 全部的 partition 整合成为一个 VG，VG 名称设定为 vbirdvg；且 PE 的大小为 16MB；
* 建立一个名为 vbirdlv 的 LV，容量大约 2G 好了！
* 最终这个 LV 格式化为 xfs 的文件系统，且挂载在 `/srv/lvm` 中  

**0.Disk 阶段（实际磁盘）**

```shell
[root@study ~]# gdisk -l /dev/vda
Number  Start (sector)    End (sector)  Size       Code  Name
   1            2048            6143   2.0 MiB     EF02
   2            6144         2103295   1024.0 MiB  0700
   3         2103296        65026047   30.0 GiB    8E00
   4        65026048        67123199   1024.0 MiB  8300  Linux filesystem
   5        67123200        69220351   1024.0 MiB  8E00  Linux LVM
   6        69220352        71317503   1024.0 MiB  8E00  Linux LVM
   7        71317504        73414655   1024.0 MiB  8E00  Linux LVM
   8        73414656        75511807   1024.0 MiB  8E00  Linux LVM
   9        75511808        77608959   1024.0 MiB  8E00  Linux LVM
# 其实 system ID 不改变也没关系！只是为了让我们管理员清楚知道该 partition 的内容，
# 所以这里建议还是修订成正确的磁盘内容较佳！
```

**PV 阶段**
* pvcreate ：将实体 partition 创建成为 PV ；
* pvscan ：搜寻目前系统里面任何具有 PV 的磁碟；
* pvdisplay ：显示出目前系统上面的 PV 状态；
* pvremove ：将 PV 属性移除，让该 partition 不具有 PV 属性。      
    
```shell
# 1. 检查有无 PV 在系统上，然后将 /dev/vda{5-8} 建立成为 PV 格式
[root@study ~]# pvscan
  PV /dev/vda3   VG centos   lvm2 [30.00 GiB / 14.00 GiB free]
  Total: 1 [30.00 GiB] / in use: 1 [30.00 GiB] / in no VG: 0 [0   ]
# 其实安装的时候，我们就有使用 LVM 了喔！所以会有 /dev/vda3 存在的！

[root@study ~]# pvcreate /dev/vda{5,6,7,8}
  Physical volume "/dev/vda5" successfully created
  Physical volume "/dev/vda6" successfully created
  Physical volume "/dev/vda7" successfully created
  Physical volume "/dev/vda8" successfully created
# 这个指令可以一口气建立这四个 partition 成为 PV 啦！注意大括号的用途

[root@study ~]# pvscan
  PV /dev/vda3   VG centos   lvm2 [30.00 GiB / 14.00 GiB free]
  PV /dev/vda8               lvm2 [1.00 GiB]
  PV /dev/vda5               lvm2 [1.00 GiB]
  PV /dev/vda7               lvm2 [1.00 GiB]
  PV /dev/vda6               lvm2 [1.00 GiB]
  Total: 5 [34.00 GiB] / in use: 1 [30.00 GiB] / in no VG: 4 [4.00 GiB]
# 这就分别显示每个 PV 的信息与系统所有 PV 的信息。尤其最后一行，显示的是：
# 整体 PV 的量 / 已经被使用到 VG 的 PV 量 / 剩余的 PV 量

# 2. 更详细的列示出系统上面每个 PV 的个别信息：
[root@study ~]# pvdisplay /dev/vda5
  "/dev/vda5" is a new physical volume of "1.00 GiB"
  --- NEW Physical volume ---
  PV Name               /dev/vda5  <==实际的 partition 装置名称
  VG Name                          <==因为尚未分配出去，所以空白！
  PV Size               1.00 GiB   <==就是容量说明
  Allocatable           NO         <==是否已被分配，结果是 NO
  PE Size               0          <==在此 PV 内的 PE 大小
  Total PE              0          <==共分区出几个 PE
  Free PE               0          <==没被 LV 用掉的 PE
  Allocated PE          0          <==尚可分配出去的 PE 数量
  PV UUID               Cb717z-lShq-6WXf-ewEj-qg0W-MieW-oAZTR6
# 由于 PE 是在建立 VG 时才给予的参数，因此在这里看到的 PV 里头的 PE 都会是 0
# 而且也没有多余的 PE 可供分配 (allocatable)。
```

**VG 阶段**

* vgcreate ：就是主要创建 VG 的命令啦！他的参数比较多，等一下介绍。
* vgscan ：搜寻系统上面是否有 VG 存在？
* vgdisplay ：显示目前系统上面的 VG 状态；
* vgextend ：在 VG 内添加额外的 PV ；
* vgreduce ：在 VG 内移除 PV；
* vgchange ：配置 VG 是否启动 (active)；
* vgremove ：删除一个 VG 啊!  
    
```shell
[root@www ~]# vgcreate [-s N[mgt]] VG名称 PV名称
选项与参数：
-s ：后面接 PE 的大小 (size) ，单位可以是 m, g, t (大小写均可)

# 1. 将 /dev/vda5-7 建立成为一个 VG，且指定 PE 为 16MB 喔！
[root@study ~]# vgcreate -s 16M vbirdvg /dev/vda{5,6,7}
  Volume group "vbirdvg" successfully created

[root@study ~]# vgscan
  Reading all physical volumes.  This may take a while...
  Found volume group "vbirdvg" using metadata type lvm2  # 我們手動製作的
  Found volume group "centos" using metadata type lvm2   # 之前系統安裝時作的

[root@study ~]# pvscan
  PV /dev/vda5   VG vbirdvg   lvm2 [1008.00 MiB / 1008.00 MiB free]
  PV /dev/vda6   VG vbirdvg   lvm2 [1008.00 MiB / 1008.00 MiB free]
  PV /dev/vda7   VG vbirdvg   lvm2 [1008.00 MiB / 1008.00 MiB free]
  PV /dev/vda3   VG centos    lvm2 [30.00 GiB / 14.00 GiB free]
  PV /dev/vda8                lvm2 [1.00 GiB]
  Total: 5 [33.95 GiB] / in use: 4 [32.95 GiB] / in no VG: 1 [1.00 GiB]
# 有三个 PV 被用去，剩下 1 个 /dev/vda8 的 PV 没被用掉！

[root@study ~]# vgdisplay vbirdvg
  --- Volume group ---
  VG Name               vbirdvg
  System ID
  Format                lvm2
  Metadata Areas        3
  Metadata Sequence No  1
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                0
  Open LV               0
  Max PV                0
  Cur PV                3
  Act PV                3
  VG Size               2.95 GiB        <==整體的 VG 容量有這麼大
  PE Size               16.00 MiB       <==內部每個 PE 的大小
  Total PE              189             <==總共的 PE 數量共有這麼多！
  Alloc PE / Size       0 / 0
  Free  PE / Size       189 / 2.95 GiB  <==尚可配置給 LV 的 PE數量/總容量有這麼多！
  VG UUID               Rx7zdR-y2cY-HuIZ-Yd2s-odU8-AkTW-okk4Ea
# 最后那三行指的就是 PE 能够使用的情况！由于尚未切出 LV，因此所有的 PE 均可自由使用。
```

增加 VG 容量  

```shell
# 2. 将剩余的 PV (/dev/vda8) 丢给 vbirdvg 吧！
[root@study ~]# vgextend vbirdvg /dev/vda8
  Volume group "vbirdvg" successfully extended

[root@study ~]# vgdisplay vbirdvg
....(前面省略)....
  VG Size               3.94 GiB
  PE Size               16.00 MiB
  Total PE              252
  Alloc PE / Size       0 / 0
  Free  PE / Size       252 / 3.94 GiB
# 基本上，不难吧！这样就可以抽换整个 VG 的大小啊！
```

**LV 阶段**

* lvcreate ：创建 LV 啦！
* lvscan ：查询系统上面的 LV ；
* lvdisplay ：显示系统上面的 LV 状态啊！
* lvextend ：在 LV 里面添加容量！
* lvreduce ：在 LV 里面减少容量；
* lvremove ：删除一个 LV ！
* lvresize ：对 LV 进行容量大小的调整！
    
```shell
[root@www ~]# lvcreate [-L N[mgt]] [-n LV名称] VG名称
[root@www ~]# lvcreate [-l N] [-n LV名称] VG名称
选项与参数：
-L  ：后面接容量，容量的单位可以是 M,G,T 等，要注意的是，最小单位为 PE，
      因此这个数量必须要是 PE 的倍数，若不相符，系统会自行计算最相近的容量。
-l  ：后面可以接 PE 的『个数』，而不是数量。若要这么做，得要自行计算 PE 数。
-n  ：后面接的就是 LV 的名称啦！
更多的说明应该可以自行查阅吧！ man lvcreate 

# 1. 将 vbirdvg 分 2GB 给 vbirdlv 喔！
[root@study ~]# lvcreate -L 2G -n vbirdlv vbirdvg
  Logical volume "vbirdlv" created
# 由于本案例中每个 PE 为 16M ，如果要用 PE 的数量来处理的话，那使用下面的指令也 OK 喔！
# lvcreate -l 128 -n vbirdlv vbirdvg

[root@study ~]# lvscan
  ACTIVE            '/dev/vbirdvg/vbirdlv' [2.00 GiB] inherit  <==新增加的一个 LV 啰！
  ACTIVE            '/dev/centos/root' [10.00 GiB] inherit
  ACTIVE            '/dev/centos/home' [5.00 GiB] inherit
  ACTIVE            '/dev/centos/swap' [1.00 GiB] inherit

[root@study ~]# lvdisplay /dev/vbirdvg/vbirdlv
  --- Logical volume ---
  LV Path                /dev/vbirdvg/vbirdlv   # 这个是 LV 的全名喔！
  LV Name                vbirdlv
  VG Name                vbirdvg
  LV UUID                QJJrTC-66sm-878Y-o2DC-nN37-2nFR-0BwMmn
  LV Write Access        read/write
  LV Creation host, time study.centos.vbird, 2015-07-28 02:22:49 +0800
  LV Status              available
  # open                 0
  LV Size                2.00 GiB               # 容量就是这么大！
  Current LE             128
  Segments               3
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     8192
  Block device           253:3
```

> **注意：** VG 的名称为 vbirdvg, 但是 LV 的名称必须使用全名！亦即是 /dev/vbirdvg/vbirdlv 才对

**文件系统阶段**

```shell
# 1. 格式化、挂载与观察我们的 LV 吧！
[root@study ~]# mkfs.xfs /dev/vbirdvg/vbirdlv <==注意 LV 全名！
[root@study ~]# mkdir /srv/lvm
[root@study ~]# mount /dev/vbirdvg/vbirdlv /srv/lvm
[root@study ~]# df -Th /srv/lvm
Filesystem                  Type  Size  Used Avail Use% Mounted on
/dev/mapper/vbirdvg-vbirdlv xfs   2.0G   33M  2.0G   2% /srv/lvm

[root@study ~]# cp -a /etc /var/log /srv/lvm
[root@study ~]# df -Th /srv/lvm
Filesystem                  Type  Size  Used Avail Use% Mounted on
/dev/mapper/vbirdvg-vbirdlv xfs   2.0G  152M  1.9G   8% /srv/lvm  <==确定是可用的啊！
```

## 14.3.3 放大 LV 容量

1. VG 阶段需要有剩余的容量
2. LV 阶段产生更多的可用容量
3. 文件系统阶段的放大

```shell
# 1. 由前面的过程我们知道 /srv/lvm 是 /dev/vbirdvg/vbirdlv 这个装置，所以检查 vbirdvg 吧！
[root@study ~]# vgdisplay vbirdvg
  --- Volume group ---
  VG Name               vbirdvg
  System ID
  Format                lvm2
  Metadata Areas        4
  Metadata Sequence No  3
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                1
  Open LV               1
  Max PV                0
  Cur PV                4
  Act PV                4
  VG Size               3.94 GiB
  PE Size               16.00 MiB
  Total PE              252
  Alloc PE / Size       128 / 2.00 GiB
  Free  PE / Size       124 / 1.94 GiB    # 看起来剩余容量确实超过 500M 的
  VG UUID               Rx7zdR-y2cY-HuIZ-Yd2s-odU8-AkTW-okk4Ea
# 既然 VG 的容量够大了！所以直接来放大 LV 吧！！

# 2. 放大 LV 吧！利用 lvresize 的功能来增加！
[root@study ~]# lvresize -L +500M /dev/vbirdvg/vbirdlv
  Rounding size to boundary between physical extents: 512.00 MiB
  Size of logical volume vbirdvg/vbirdlv changed from 2.00 GiB (128 extents) to 2.50 GiB 
(160 extents).
  Logical volume vbirdlv successfully resized
# 这样就增加了 LV 了喔！lvresize 的语法很简单，基本上同样透过 -l 或 -L 来增加！
# 若要增加则使用 + ，若要减少则使用 - ！详细的选项请参考 man lvresize 啰！

[root@study ~]# lvscan
  ACTIVE            '/dev/vbirdvg/vbirdlv' [2.50 GiB] inherit
  ACTIVE            '/dev/centos/root' [10.00 GiB] inherit
  ACTIVE            '/dev/centos/home' [5.00 GiB] inherit
  ACTIVE            '/dev/centos/swap' [1.00 GiB] inherit
# 可以发现 /dev/vbirdvg/vbirdlv 容量由 2G 增加到 2.5G 啰！

[root@study ~]# df -Th /srv/lvm
Filesystem                  Type  Size  Used Avail Use% Mounted on
/dev/mapper/vbirdvg-vbirdlv xfs   2.0G  111M  1.9G   6% /srv/lvm
```

```shell
# 3.1 先看一下原本的文件系统内的 superblock 记录情况吧！
[root@study ~]# xfs_info /srv/lvm
meta-data=/dev/mapper/vbirdvg-vbirdlv isize=256    agcount=4, agsize=131072 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=0        finobt=0
data     =                       bsize=4096   blocks=524288, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
log      =internal               bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0

[root@study ~]# xfs_growfs /srv/lvm  # 这一步骤才是最重要的！
[root@study ~]# xfs_info /srv/lvm
meta-data=/dev/mapper/vbirdvg-vbirdlv isize=256    agcount=5, agsize=131072 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=0        finobt=0
data     =                       bsize=4096   blocks=655360, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
log      =internal               bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0

[root@study ~]# df -Th /srv/lvm
Filesystem                  Type  Size  Used Avail Use% Mounted on
/dev/mapper/vbirdvg-vbirdlv xfs   2.5G  111M  2.4G   5% /srv/lvm

[root@study ~]# ls -l /srv/lvm
drwxr-xr-x. 131 root root 8192 Jul 28 00:12 etc
drwxr-xr-x.  16 root root 4096 Jul 28 00:01 log
# 刚刚复制进去的数据可还是存在的喔！并没有消失不见！
```

## 14.3.4 缩小 LV 容量

```shell 
# 1. 先找出 /dev/hda6 的容量大小，并尝试计算文件系统需缩小到多少
[root@www ~]# pvdisplay
  --- Physical volume ---
  PV Name               /dev/hda6
  VG Name               vbirdvg
  PV Size               1.40 GB / not usable 11.46 MB
  Allocatable           yes (but full)
  PE Size (KByte)       16384
  Total PE              89
  Free PE               0
  Allocated PE          89
  PV UUID               Z13Jk5-RCls-UJ8B-HzDa-Gesn-atku-rf2biN
# 从这里可以看出 /dev/hda6 有多大，而且含有 89 个 PE 的量喔！
# 那如果要使用 resize2fs 时，则总量减去 1.40GB 就对了！

[root@www ~]# pvscan
  PV /dev/hda6    VG vbirdvg   lvm2 [1.39 GB / 0    free]
  PV /dev/hda7    VG vbirdvg   lvm2 [1.39 GB / 0    free]
  PV /dev/hda8    VG vbirdvg   lvm2 [1.39 GB / 0    free]
  PV /dev/hda9    VG vbirdvg   lvm2 [1.39 GB / 0    free]
  PV /dev/hda10   VG vbirdvg   lvm2 [2.80 GB / 0    free]
  Total: 5 [8.36 GB] / in use: 5 [8.36 GB] / in no VG: 0 [0   ]
# 从上面可以发现如果扣除 /dev/hda6 则剩余容量有：1.39*3+2.8=6.97

# 2. 就直接降低文件系统的容量吧！
[root@www ~]# resize2fs /dev/vbirdvg/vbirdlv 6900M
resize2fs 1.39 (29-May-2006)
Filesystem at /dev/vbirdvg/vbirdlv is mounted on /mnt/lvm; on-line resizing
On-line shrinking from 2191360 to 1766400 not supported.
# 容量好像不能够写小数点位数，因此 6.9G 是错误的，鸟哥就使用 6900M 了。
# 此外，放大可以线上直接进行，缩小文件系统似乎无法支持！所以要这样做：

[root@www ~]# umount /mnt/lvm
[root@www ~]# resize2fs /dev/vbirdvg/vbirdlv 6900M
resize2fs 1.39 (29-May-2006)
Please run 'e2fsck -f /dev/vbirdvg/vbirdlv' first.
# 他要我们先进行磁碟检查！不罗唆！那就直接进行吧！

[root@www ~]# e2fsck -f /dev/vbirdvg/vbirdlv
e2fsck 1.39 (29-May-2006)
Pass 1: Checking inodes, blocks, and sizes
Pass 2: Checking directory structure
Pass 3: Checking directory connectivity
Pass 4: Checking reference counts
Pass 5: Checking group summary information
/dev/vbirdvg/vbirdlv: 2438/1087008 files (0.1% non-contiguous), 

[root@www ~]# resize2fs /dev/vbirdvg/vbirdlv 6900M
resize2fs 1.39 (29-May-2006)
Resizing the filesystem on /dev/vbirdvg/vbirdlv to 1766400 (4k) blocks.
The filesystem on /dev/vbirdvg/vbirdlv is now 1766400 blocks long.
# 再来 resize2fs 一次就能够成功了！如上所示啊！

[root@www ~]# mount /dev/vbirdvg/vbirdlv /mnt/lvm
[root@www ~]# df /mnt/lvm
Filesystem           1K-blocks      Used Available Use% Mounted on
/dev/mapper/vbirdvg-vbirdlv
                       6955584    262632   6410328   4% /mnt/lvm
```

```shell 
# 3. 降低 LV 的容量，同时我们知道 /dev/hda6 有 89 个 PE
[root@www ~]# lvresize -l -89 /dev/vbirdvg/vbirdlv
  WARNING: Reducing active and open logical volume to 6.97 GB
  THIS MAY DESTROY YOUR DATA (filesystem etc.)
Do you really want to reduce vbirdlv? [y/n]: y
  Reducing logical volume vbirdlv to 6.97 GB
  Logical volume vbirdlv successfully resized
# 会有警告信息！但是我们的实际数据量还是比 6.97G 小，所以就 y 下去吧！

[root@www ~]# lvdisplay
  --- Logical volume ---
  LV Name                /dev/vbirdvg/vbirdlv
  VG Name                vbirdvg
  LV UUID                8vFOPG-Jrw0-Runh-ug24-t2j7-i3nA-rPEyq0
  LV Write Access        read/write
  LV Status              available
  # open                 1
  LV Size                6.97 GB
  Current LE             446
  Segments               5
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           253:0
```

```shell 
# 4.1 先确认 /dev/hda6 是否将 PE 都移除了！
[root@www ~]# pvdisplay
  --- Physical volume ---
  PV Name               /dev/hda6
  VG Name               vbirdvg
  PV Size               1.40 GB / not usable 11.46 MB
  Allocatable           yes (but full)
  PE Size (KByte)       16384
  Total PE              89
  Free PE               0
  Allocated PE          89
  PV UUID               Z13Jk5-RCls-UJ8B-HzDa-Gesn-atku-rf2biN
....(中间省略)....

  --- Physical volume ---
  PV Name               /dev/hda10
  VG Name               vbirdvg
  PV Size               2.80 GB / not usable 6.96 MB
  Allocatable           yes
  PE Size (KByte)       16384
  Total PE              179
  Free PE               89
  Allocated PE          90
  PV UUID               7MfcG7-y9or-0Jmb-H7RO-5Pa5-D3qB-G426Vq
# 搞了老半天，没有被使用的 PE 竟然在 /dev/hda10 ！此时得要搬移 PE 罗！

[root@www ~]# pvmove /dev/hda6 /dev/hda10
# pvmove 来源PV 目标PV ，可以将 /dev/hda6 内的 PE 通通移动到 /dev/hda10
# 尚未被使用的 PE 去 (Free PE)。

# 4.2 将 /dev/hda6 移出 vbirdvg 中！
[root@www ~]# vgreduce vbirdvg /dev/hda6
  Removed "/dev/hda6" from volume group "vbirdvg"

[root@www ~]# pvscan
  PV /dev/hda7    VG vbirdvg   lvm2 [1.39 GB / 0    free]
  PV /dev/hda8    VG vbirdvg   lvm2 [1.39 GB / 0    free]
  PV /dev/hda9    VG vbirdvg   lvm2 [1.39 GB / 0    free]
  PV /dev/hda10   VG vbirdvg   lvm2 [2.80 GB / 0    free]
  PV /dev/hda6                 lvm2 [1.40 GB]
  Total: 5 [8.37 GB] / in use: 4 [6.97 GB] / in no VG: 1 [1.40 GB]

[root@www ~]# pvremove /dev/hda6
  Labels on physical volume "/dev/hda6" successfully wiped
```

## 14.3.5 使用 LVM thin Volume 让 LVM 动态自动调整磁盘使用率

```shell
# 1. 先以 lvcreate 来建立 vbirdtpool 这个 thin pool 装置：
[root@study ~]# lvcreate -L 1G -T vbirdvg/vbirdtpool  # 最重要的建置指令
[root@study ~]# lvdisplay /dev/vbirdvg/vbirdtpool
  --- Logical volume ---
  LV Name                vbirdtpool
  VG Name                vbirdvg
  LV UUID                p3sLAg-Z8jT-tBuT-wmEL-1wKZ-jrGP-0xmLtk
  LV Write Access        read/write
  LV Creation host, time study.centos.vbird, 2015-07-28 18:27:32 +0800
  LV Pool metadata       vbirdtpool_tmeta
  LV Pool data           vbirdtpool_tdata
  LV Status              available
  # open                 0
  LV Size                1.00 GiB   # 总共可分配出去的容量
  Allocated pool data    0.00%      # 已分配的容量百分比
  Allocated metadata     0.24%      # 已分配的中介数据百分比
  Current LE             64
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     8192
  Block device           253:6
# 非常有趣吧！竟然在 LV 装置中还可以有再分配 (Allocated) 的项目耶！果然是储存池！

[root@study ~]# lvs vbirdvg  # 语法为 lvs VGname
  LV         VG      Attr       LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  vbirdlv    vbirdvg -wi-ao---- 2.50g
  vbirdtpool vbirdvg twi-a-tz-- 1.00g             0.00   0.24
# 这个 lvs 指令的输出更加简单明了！直接看比较清晰！

# 2. 开始建立 vbirdthin1 这个有 10GB 的装置，注意！必须使用 --thin 与 vbirdtpool 连结喔！
[root@study ~]# lvcreate -V 10G -T vbirdvg/vbirdtpool -n vbirdthin1

[root@study ~]# lvs vbirdvg
  LV         VG      Attr       LSize  Pool       Origin Data%  Meta%  Move Log Cpy%Sync Convert
  vbirdlv    vbirdvg -wi-ao----  2.50g
  vbirdthin1 vbirdvg Vwi-a-tz-- 10.00g vbirdtpool        0.00
  vbirdtpool vbirdvg twi-aotz--  1.00g                   0.00   0.27
# 很有趣吧！明明连 vbirdvg 这个 VG 都没有足够大到 10GB 的容量，透过 thin pool
# 竟然就产生了 10GB 的 vbirdthin1 这个装置了！好有趣！

# 3. 开始建立文件系统
[root@study ~]# mkfs.xfs /dev/vbirdvg/vbirdthin1
[root@study ~]# mkdir /srv/thin
[root@study ~]# mount /dev/vbirdvg/vbirdthin1 /srv/thin
[root@study ~]# df -Th /srv/thin
Filesystem                     Type  Size  Used Avail Use% Mounted on
/dev/mapper/vbirdvg-vbirdthin1 xfs    10G   33M   10G   1% /srv/thin
# 真的有 10GB 耶！！

# 4. 测试一下容量的使用！建立 500MB 的文件，但不可超过 1GB 的测试为宜！
[root@study ~]# dd if=/dev/zero of=/srv/thin/test.img bs=1M count=500
[root@study ~]# lvs vbirdvg
  LV         VG      Attr       LSize  Pool       Origin Data%  Meta%  Move Log Cpy%Sync Convert
  vbirdlv    vbirdvg -wi-ao----  2.50g
  vbirdthin1 vbirdvg Vwi-aotz-- 10.00g vbirdtpool        4.99
  vbirdtpool vbirdvg twi-aotz--  1.00g                   49.93  1.81
# 很要命！这时已经分配出 49% 以上的容量了！而 vbirdthin1 却只看到用掉 5% 而已！
# 所以鸟哥认为，这个 thin pool 非常好用！但是在管理上，得要特别特别的留意！
```

> 实际上，真的可用容量就是实际的磁盘储存池内的容量！ 如果突破该容量，这个 thin pool 可是会爆炸而让资料损毁的！

## 14.3.6 LVM 的系统快照

**传统快照区的建立**

```shell
# 1. 先观察 VG 还剩下多少剩余容量
[root@study ~]# vgdisplay vbirdvg
....(其他省略)....
  Total PE              252
  Alloc PE / Size       226 / 3.53 GiB
  Free  PE / Size       26 / 416.00 MiB
# 就只有剩下 26 个 PE 了！全部分配给 vbirdsnap1 啰！

# 2. 利用 lvcreate 建立 vbirdlv 的快照区，快照被取名为 vbirdsnap1，且给予 26 个 PE
[root@study ~]# lvcreate -s -l 26 -n vbirdsnap1 /dev/vbirdvg/vbirdlv
  Logical volume "vbirdsnap1" created
# 上述的指令中最重要的是那个 -s 的选项！代表是 snapshot 快照功能之意！
# -n 后面接快照区的装置名称， /dev/.... 则是要被快照的 LV 完整檔名。
# -l 后面则是接使用多少个 PE 来作为这个快照区使用。

[root@study ~]# lvdisplay /dev/vbirdvg/vbirdsnap1
  --- Logical volume ---
  LV Path                /dev/vbirdvg/vbirdsnap1
  LV Name                vbirdsnap1
  VG Name                vbirdvg
  LV UUID                I3m3Oc-RIvC-unag-DiiA-iQgI-I3z9-0OaOzR
  LV Write Access        read/write
  LV Creation host, time study.centos.vbird, 2015-07-28 19:21:44 +0800
  LV snapshot status     active destination for vbirdlv
  LV Status              available
  # open                 0
  LV Size                2.50 GiB    # 原始碟，就是 vbirdlv 的原始容量
  Current LE             160
  COW-table size         416.00 MiB  # 这个快照能够纪录的最大容量！
  COW-table LE           26
  Allocated to snapshot  0.00%       # 目前已经被用掉的容量！
  Snapshot chunk size    4.00 KiB
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     8192
  Block device           253:11
```

```shell
[root@study ~]# mkdir /srv/snapshot1
[root@study ~]# mount -o nouuid /dev/vbirdvg/vbirdsnap1 /srv/snapshot1
[root@study ~]# df -Th /srv/lvm /srv/snapshot1
Filesystem                     Type  Size  Used Avail Use% Mounted on
/dev/mapper/vbirdvg-vbirdlv    xfs   2.5G  111M  2.4G   5% /srv/lvm
/dev/mapper/vbirdvg-vbirdsnap1 xfs   2.5G  111M  2.4G   5% /srv/snapshot1
# 有没有看到！这两个咚咚竟然是一模一样喔！我们根本没有动过
# /dev/vbirdvg/vbirdsnap1 对吧！不过这里面会主动记录原 vbirdlv 的内容！
```

> 因为 XFS 不允许相同的 UUID 文件系统的挂载，因此我们得要加上那个 nouuid 的参数，让文件系统忽略相同的 UUID 所造成的问题！

**利用快照区复原原系统**

>要复原的数据量不能够高于快照区所能负载的实际容量  

```shell
# 1. 先将原本的 /dev/vbirdvg/vbirdlv 内容作些变更，增增减减一些目录吧！
[root@study ~]# df -Th /srv/lvm /srv/snapshot1
Filesystem                     Type  Size  Used Avail Use% Mounted on
/dev/mapper/vbirdvg-vbirdlv    xfs   2.5G  111M  2.4G   5% /srv/lvm
/dev/mapper/vbirdvg-vbirdsnap1 xfs   2.5G  111M  2.4G   5% /srv/snapshot1

[root@study ~]# cp -a /usr/share/doc /srv/lvm
[root@study ~]# rm -rf /srv/lvm/log
[root@study ~]# rm -rf /srv/lvm/etc/sysconfig
[root@study ~]# df -Th /srv/lvm /srv/snapshot1
Filesystem                     Type  Size  Used Avail Use% Mounted on
/dev/mapper/vbirdvg-vbirdlv    xfs   2.5G  146M  2.4G   6% /srv/lvm
/dev/mapper/vbirdvg-vbirdsnap1 xfs   2.5G  111M  2.4G   5% /srv/snapshot1
[root@study ~]# ll /srv/lvm /srv/snapshot1
/srv/lvm:
total 60
drwxr-xr-x. 887 root root 28672 Jul 20 23:03 doc
drwxr-xr-x. 131 root root  8192 Jul 28 00:12 etc

/srv/snapshot1:
total 16
drwxr-xr-x. 131 root root 8192 Jul 28 00:12 etc
drwxr-xr-x.  16 root root 4096 Jul 28 00:01 log
# 两个目录的内容看起来已经不太一样了喔！检测一下快照 LV 吧！

[root@study ~]# lvdisplay /dev/vbirdvg/vbirdsnap1
  --- Logical volume ---
  LV Path                /dev/vbirdvg/vbirdsnap1
....(中間省略)....
  Allocated to snapshot  21.47%
# 鸟哥仅列出最重要的部份！就是全部的容量已经被用掉了 21.4% 啰！

# 2. 利用快照区将原本的 filesystem 备份，我们使用 xfsdump 来处理！
[root@study ~]# xfsdump -l 0 -L lvm1 -M lvm1 -f /home/lvm.dump /srv/snapshot1
# 此时你就会有一个备份资料，亦即是 /home/lvm.dump 了！
```

```shell
# 3. 将 vbirdsnap1 卸除并移除 (因为里面的内容已经备份起来了)
[root@study ~]# umount /srv/snapshot1
[root@study ~]# lvremove /dev/vbirdvg/vbirdsnap1
Do you really want to remove active logical volume "vbirdsnap1"? [y/n]: y
  Logical volume "vbirdsnap1" successfully removed

[root@study ~]# umount /srv/lvm
[root@study ~]# mkfs.xfs -f /dev/vbirdvg/vbirdlv
[root@study ~]# mount /dev/vbirdvg/vbirdlv /srv/lvm
[root@study ~]# xfsrestore -f /home/lvm.dump -L lvm1 /srv/lvm
[root@study ~]# ll /srv/lvm
drwxr-xr-x. 131 root root 8192 Jul 28 00:12 etc
drwxr-xr-x.  16 root root 4096 Jul 28 00:01 log
# 是否与最初的内容相同啊！这就是透过快照来还原的一个简单的方法啰！
```
	
**利用快照区进行各项练习与测试的任务，再以原系统还原快照**

将原本的 vbirdlv 当作备份数据，然后将 vbirdss 当作实际在运行中的数据， 任何测试的动作都在 vbirdss 这个快照区当中测试，那么当测试完毕要将测试的数据删除时，只要将快照区删去即可！ 而要复制一个 vbirdlv 的系统，再作另外一个快照区即可！

```shell
# 1. 创建一个大一些的快照区，让我们将 /dev/hda6 的 PE 全部给快照区！
[root@www ~]# lvcreate -s -l 89 -n vbirdss /dev/vbirdvg/vbirdlv
  Logical volume "vbirdss" created

[root@www ~]# lvdisplay /dev/vbirdvg/vbirdss
  --- Logical volume ---
  LV Name                /dev/vbirdvg/vbirdss
  VG Name                vbirdvg
  LV UUID                as0ocQ-KjRS-Bu7y-fYoD-1CHC-0V3Y-JYsjj1
  LV Write Access        read/write
  LV snapshot status     active destination for /dev/vbirdvg/vbirdlv
  LV Status              available
  # open                 0
  LV Size                6.97 GB
  Current LE             446
  COW-table size         1.39 GB
  COW-table LE           89
  Allocated to snapshot  0.00%
  Snapshot chunk size    4.00 KB
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           253:1
# 如何！这个快照区不小吧！

# 2. 隐藏 vbirdlv 挂载 vbirdss
[root@www ~]# umount /mnt/lvm
[root@www ~]# mount /dev/vbirdvg/vbirdss /mnt/snapshot
[root@www ~]# df /mnt/snapshot
Filesystem           1K-blocks      Used Available Use% Mounted on
/dev/mapper/vbirdvg-vbirdss
                       7192504    265804   6561340   4% /mnt/snapshot

# 3. 开始恶搞！
[root@www ~]# rm -r /mnt/snapshot/etc /mnt/snapshot/log
[root@www ~]# cp -a /boot /lib /sbin /mnt/snapshot/
[root@www ~]# ll /mnt/snapshot
drwxr-xr-x  4 root root  4096 Dec 15 16:28 boot
drwxr-xr-x 14 root root  4096 Sep  5  2008 lib
drwx------  2 root root 16384 Mar 11 16:59 lost+found
drwxr-xr-x  2 root root 12288 Sep  5  2008 sbin  <==与原本数据有差异了

[root@www ~]# mount /dev/vbirdvg/vbirdlv /mnt/lvm
[root@www ~]# ll /mnt/lvm
drwxr-xr-x 105 root root 12288 Mar 11 16:59 etc
drwxr-xr-x  17 root root  4096 Mar 11 14:17 log
drwx------   2 root root 16384 Mar 11 16:59 lost+found
# 不论你在快照区恶搞啥咚咚，原本的 vbirdlv 里面的数据安好如初啊！
# 假设你将 vbirdss 搞烂了！里面的数据不再需要！那该如何是好？

# 4. 还原原本快照区的数据，回到与原文件系统相同的资讯
[root@www ~]# umount /mnt/snapshot
[root@www ~]# lvremove /dev/vbirdvg/vbirdss
Do you really want to remove active logical volume "vbirdss"? [y/n]: y
  Logical volume "vbirdss" successfully removed

[root@www ~]# lvcreate -s -l 89 -n vbirdss /dev/vbirdvg/vbirdlv
[root@www ~]# mount /dev/vbirdvg/vbirdss /mnt/snapshot
[root@www ~]# ll /mnt/snapshot
drwxr-xr-x 105 root root 12288 Mar 11 16:59 etc
drwxr-xr-x  17 root root  4096 Mar 11 14:17 log
drwx------   2 root root 16384 Mar 11 16:59 lost+found
# 数据这样就复原了！
```	


## 14.3.6 LVM 相关命令汇整与 LVM 的关闭

任务 | PV 阶段 | VG 阶段 | LV 阶段  | XFS | EXT4
:--- | :--- | :--- | :--- | :--- |  :---  | :---
查找(scan) | pvscan | 	vgscan | 	lvscan |  lsblk, blkid
新建(create) | pvcreate | vgcreate | lvcreate | mkfs.xfs  | mkfs.ext4
显示(display) | 	pvdisplay | vgdisplay | lvdisplay | df, mount
添加(extend) |  | vgextend | lvextend (lvresize) |  xfs_growfs  | resize2fs
减少(reduce) |  | vgreduce | lvreduce (lvresize) |  不支持  | resize2fs
删除(remove) | pvremove | vgremove | lvremove | umount, 重新格式化
改变容量(resize) |  |  | lvresize | xfs_growfs  | resize2fs
改变属性(attribute) | pvchange | vgchange | lvchange |  /etc/fstab, remount

* 先卸载系统上面的 LVM 文件系统 (包括快照与所有 LV)；
* 使用 lvremove 移除 LV ；
* 使用 vgchange -a n VGname 让 VGname 这个 VG 不具有 Active 的标志；
* 使用 vgremove 移除 VG：
* 使用 pvremove 移除 PV；
* 最后，使用 fdisk 修改 ID 回来啊！

```shell
[root@www ~]# umount /mnt/lvm
[root@www ~]# umount /mnt/snapshot
[root@www ~]# lvremove /dev/vbirdvg/vbirdss  <==先处理快照
Do you really want to remove active logical volume "vbirdss"? [y/n]: y
  Logical volume "vbirdss" successfully removed
[root@www ~]# lvremove /dev/vbirdvg/vbirdlv  <==再处理原系统
Do you really want to remove active logical volume "vbirdlv"? [y/n]: y
  Logical volume "vbirdlv" successfully removed

[root@www ~]# vgchange -a n vbirdvg
  0 logical volume(s) in volume group "vbirdvg" now active

[root@www ~]# vgremove vbirdvg
  Volume group "vbirdvg" successfully removed

[root@www ~]# pvremove /dev/hda{6,7,8,9,10}
  Labels on physical volume "/dev/hda6" successfully wiped
  Labels on physical volume "/dev/hda7" successfully wiped
  Labels on physical volume "/dev/hda8" successfully wiped
  Labels on physical volume "/dev/hda9" successfully wiped
  Labels on physical volume "/dev/hda10" successfully wiped
```